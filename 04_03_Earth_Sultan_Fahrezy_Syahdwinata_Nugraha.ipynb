{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KyRMRLWRa-Z8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.datasets as datasets\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "training_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=4, shuffle=True)\n",
        "validation_loader = torch.utils.data.DataLoader(mnist_testset, batch_size=4, shuffle=False)"
      ],
      "metadata": {
        "id": "dXxhijozoaP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(mnist_trainset)\n",
        "type(mnist_trainset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KMjMUeMpemU",
        "outputId": "8aa953cb-b112-44f7-e940-9fd71ac3f034"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torchvision.datasets.mnist.MNIST"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# You can put between 0 to 59999 here\n",
        "index = 40\n",
        "\n",
        "# Set number of characters per row when printing\n",
        "np.set_printoptions(linewidth=320)\n",
        "\n",
        "# Print the label and image\n",
        "print(f'LABEL: {mnist_trainset.__getitem__(index)[1]}')\n",
        "print(f'\\nIMAGE PIXEL ARRAY:\\n {mnist_trainset.__getitem__(index)[0]}')\n",
        "\n",
        "# Visualize the image\n",
        "plt.imshow(mnist_trainset.__getitem__(index)[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "id": "qutqK3FXozyG",
        "outputId": "36ec2251-5ce7-45c3-d44b-cd1cfb47b6a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LABEL: 1\n",
            "\n",
            "IMAGE PIXEL ARRAY:\n",
            " <PIL.Image.Image image mode=L size=28x28 at 0x7CB60CFBA350>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7cb60d0306d0>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Me5Ah0jhqCQk",
        "outputId": "e260cf0d-a65f-4d43-d5df-5a61b458457b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512,10),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork()"
      ],
      "metadata": {
        "id": "5WIBApL5qrjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
      ],
      "metadata": {
        "id": "jeyCFYRBtn26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(epoch_index, tb_writer):\n",
        "    running_loss = 0.\n",
        "    last_loss = 0.\n",
        "    acc = 0.\n",
        "\n",
        "    # Here, we use enumerate(training_loader) instead of\n",
        "    # iter(training_loader) so that we can track the batch\n",
        "    # index and do some intra-epoch reporting\n",
        "    for i, data in enumerate(training_loader):\n",
        "        # Every data instance is an input + label pair\n",
        "        inputs, labels = data\n",
        "\n",
        "        # Zero your gradients for every batch!\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Make predictions for this batch\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Compute the loss and its gradients\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "\n",
        "        # Adjust learning weights\n",
        "        optimizer.step()\n",
        "\n",
        "        # Gather data and report\n",
        "        running_loss += loss.item()\n",
        "        if i % 1000 == 999:\n",
        "            last_loss = running_loss / 1000 # loss per batch\n",
        "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
        "            tb_x = epoch_index * len(training_loader) + i + 1\n",
        "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
        "            running_loss = 0.\n",
        "\n",
        "    return last_loss"
      ],
      "metadata": {
        "id": "RygB1zKTtt6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from datetime import datetime\n",
        "\n",
        "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
        "epoch_number = 0\n",
        "\n",
        "EPOCHS = 5\n",
        "\n",
        "best_vloss = 1_000_000.\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print('EPOCH {}:'.format(epoch_number + 1))\n",
        "\n",
        "    # Make sure gradient tracking is on, and do a pass over the data\n",
        "    model.train(True)\n",
        "    avg_loss = train_one_epoch(epoch_number, writer)\n",
        "\n",
        "\n",
        "    running_vloss = 0.0\n",
        "    # Set the model to evaluation mode, disabling dropout and using population\n",
        "    # statistics for batch normalization.\n",
        "    model.eval()\n",
        "\n",
        "    # Disable gradient computation and reduce memory consumption.\n",
        "    with torch.no_grad():\n",
        "        for i, vdata in enumerate(validation_loader):\n",
        "            vinputs, vlabels = vdata\n",
        "            voutputs = model(vinputs)\n",
        "            vloss = loss_fn(voutputs, vlabels)\n",
        "            running_vloss += vloss\n",
        "\n",
        "    avg_vloss = running_vloss / (i + 1)\n",
        "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
        "\n",
        "    # Log the running loss averaged per batch\n",
        "    # for both training and validation\n",
        "    writer.add_scalars('Training vs. Validation Loss',\n",
        "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
        "                    epoch_number + 1)\n",
        "    writer.flush()\n",
        "\n",
        "    # Track best performance, and save the model's state\n",
        "    if avg_vloss < best_vloss:\n",
        "        best_vloss = avg_vloss\n",
        "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "\n",
        "    epoch_number += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAaZaIucuCmI",
        "outputId": "034178ee-e1b9-4052-a24b-73ae8092efb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 1:\n",
            "  batch 1000 loss: 2.3022861025333405\n",
            "  batch 2000 loss: 2.301105772972107\n",
            "  batch 3000 loss: 2.298769552707672\n",
            "  batch 4000 loss: 2.3012812614440916\n",
            "  batch 5000 loss: 2.2985592193603517\n",
            "  batch 6000 loss: 2.3014400453567503\n",
            "  batch 7000 loss: 2.3012797544002535\n",
            "  batch 8000 loss: 2.301996893167496\n",
            "  batch 9000 loss: 2.3013648171424865\n",
            "  batch 10000 loss: 2.3034223799705504\n",
            "  batch 11000 loss: 2.299433210372925\n",
            "  batch 12000 loss: 2.3035750155448915\n",
            "  batch 13000 loss: 2.3031474494934083\n",
            "  batch 14000 loss: 2.3017159514427186\n",
            "  batch 15000 loss: 2.3036071438789367\n",
            "LOSS train 2.3036071438789367 valid 2.299274444580078\n",
            "EPOCH 2:\n",
            "  batch 1000 loss: 2.300747334241867\n",
            "  batch 2000 loss: 2.3002480518817903\n",
            "  batch 3000 loss: 2.3007548253536223\n",
            "  batch 4000 loss: 2.303999205350876\n",
            "  batch 5000 loss: 2.29958197593689\n",
            "  batch 6000 loss: 2.301960282325745\n",
            "  batch 7000 loss: 2.3033778479099274\n",
            "  batch 8000 loss: 2.2998592534065248\n",
            "  batch 9000 loss: 2.302435200214386\n",
            "  batch 10000 loss: 2.301231376886368\n",
            "  batch 11000 loss: 2.3009616622924804\n",
            "  batch 12000 loss: 2.3027300193309785\n",
            "  batch 13000 loss: 2.299799087047577\n",
            "  batch 14000 loss: 2.3025413177013396\n",
            "  batch 15000 loss: 2.302757132768631\n",
            "LOSS train 2.302757132768631 valid 2.299274444580078\n",
            "EPOCH 3:\n",
            "  batch 1000 loss: 2.299206057548523\n",
            "  batch 2000 loss: 2.302822104215622\n",
            "  batch 3000 loss: 2.297942744255066\n",
            "  batch 4000 loss: 2.3000231738090515\n",
            "  batch 5000 loss: 2.302243038415909\n",
            "  batch 6000 loss: 2.3039292640686035\n",
            "  batch 7000 loss: 2.3012550728321077\n",
            "  batch 8000 loss: 2.303806398630142\n",
            "  batch 9000 loss: 2.3002686417102813\n",
            "  batch 10000 loss: 2.297823537826538\n",
            "  batch 11000 loss: 2.302118198633194\n",
            "  batch 12000 loss: 2.3025755078792574\n",
            "  batch 13000 loss: 2.3019791362285615\n",
            "  batch 14000 loss: 2.301721515417099\n",
            "  batch 15000 loss: 2.305270170211792\n",
            "LOSS train 2.305270170211792 valid 2.299274444580078\n",
            "EPOCH 4:\n",
            "  batch 1000 loss: 2.2999976806640623\n",
            "  batch 2000 loss: 2.302595042228699\n",
            "  batch 3000 loss: 2.3002073616981504\n",
            "  batch 4000 loss: 2.299992346048355\n",
            "  batch 5000 loss: 2.3007212665081025\n",
            "  batch 6000 loss: 2.3037024145126344\n",
            "  batch 7000 loss: 2.2994463090896606\n",
            "  batch 8000 loss: 2.302000725030899\n",
            "  batch 9000 loss: 2.3011480722427367\n",
            "  batch 10000 loss: 2.299357573032379\n",
            "  batch 11000 loss: 2.304637956380844\n",
            "  batch 12000 loss: 2.301653563261032\n",
            "  batch 13000 loss: 2.302115475654602\n",
            "  batch 14000 loss: 2.300878666162491\n",
            "  batch 15000 loss: 2.304530127286911\n",
            "LOSS train 2.304530127286911 valid 2.299274444580078\n",
            "EPOCH 5:\n",
            "  batch 1000 loss: 2.300270369529724\n",
            "  batch 2000 loss: 2.299898356437683\n",
            "  batch 3000 loss: 2.301977374315262\n",
            "  batch 4000 loss: 2.3007589416503906\n",
            "  batch 5000 loss: 2.302575032711029\n",
            "  batch 6000 loss: 2.302707800865173\n",
            "  batch 7000 loss: 2.3041095480918883\n",
            "  batch 8000 loss: 2.3033460080623627\n",
            "  batch 9000 loss: 2.300249722480774\n",
            "  batch 10000 loss: 2.3004837250709533\n",
            "  batch 11000 loss: 2.29890850520134\n",
            "  batch 12000 loss: 2.303298555850983\n",
            "  batch 13000 loss: 2.300778213262558\n",
            "  batch 14000 loss: 2.302975322008133\n",
            "  batch 15000 loss: 2.3006471185684205\n",
            "LOSS train 2.3006471185684205 valid 2.299274444580078\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Penjelasan Model\n",
        "Ini merupakan model squensial dengan input linear 28*28 dan output linear juga dengan 10 kelas output sesuai dengan kelas MNIST. Terdapat hidden layer ReLU dan Linear pada model ini. Model ini menggunakan loss function CrossEntropy dan Optimizer SGD dengan learning rate 0.001 dan momenntum 0.9. Model ini ditrain sebanyak 5 kali epoch dengan satu epoch memiliki jumlah batch 15000, karena satu batch terdapat 4 data."
      ],
      "metadata": {
        "id": "LaqLiDtgbbt9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p6Gz8lDwcRQB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}